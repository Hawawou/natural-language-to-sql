{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Salesforce/wikisql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forma data \n",
    "system_message = \"\"\"You are a natural language to sql query translator model. Users will ask you a question in English and you will generate a SQL query based on the table provided: {table}\"\"\"\n",
    "\n",
    "def format_data(dataset):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message.format(table=dataset[\"table\"])},\n",
    "            {\"role\": \"user\", \"content\": dataset[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": dataset[\"sql\"][\"human_readable\"]}\n",
    "        ]}\n",
    "\n",
    "def format_schema(table):\n",
    "    headers = \" , \".join(table[\"header\"])\n",
    "    return f\"Table ID: {table['id']}, Columns: [{headers}]\"\n",
    "\n",
    "train_data = dataset['train'].map(format_data)\n",
    "val_data = dataset['validation'].map(format_data)\n",
    "test_data = dataset['test'].map(format_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_data)\n",
    "df2 = pd.DataFrame(val_data)\n",
    "df3 = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[[\"messages\"]]\n",
    "val_data = df2[[\"messages\"]]\n",
    "test_data = df3[[\"messages\"]]\n",
    "train_data.to_json(\"train_data.jsonl\", orient=\"records\", lines=True)\n",
    "val_data.to_json(\"val_data.jsonl\", orient=\"records\", lines=True)\n",
    "test_data.to_json(\"test_data.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def open_json(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': \"You are a natural language to sql query translator model. Users will ask you a question in English and you will generate a SQL query based on the table provided: {'header': ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes'], 'page_title': '', 'page_id': '', 'types': ['text', 'text', 'text', 'text', 'text', 'text'], 'id': '1-1000181-1', 'section_title': '', 'caption': '', 'rows': [['Australian Capital Territory', 'blue/white', 'Yaa·nna', 'ACT · CELEBRATION OF A CENTURY 2013', 'YIL·00A', 'Slogan screenprinted on plate'], ['New South Wales', 'black/yellow', 'aa·nn·aa', 'NEW SOUTH WALES', 'BX·99·HI', 'No slogan on current series'], ['New South Wales', 'black/white', 'aaa·nna', 'NSW', 'CPX·12A', 'Optional white slimline series'], ['Northern Territory', 'ochre/white', 'Ca·nn·aa', 'NT · OUTBACK AUSTRALIA', 'CB·06·ZZ', 'New series began in June 2011'], ['Queensland', 'maroon/white', 'nnn·aaa', 'QUEENSLAND · SUNSHINE STATE', '999·TLG', 'Slogan embossed on plate'], ['South Australia', 'black/white', 'Snnn·aaa', 'SOUTH AUSTRALIA', 'S000·AZD', 'No slogan on current series'], ['Victoria', 'blue/white', 'aaa·nnn', 'VICTORIA - THE PLACE TO BE', 'ZZZ·562', 'Current series will be exhausted this year']], 'name': 'table_1000181_1'}\", 'role': 'system'}, {'content': 'Tell me what the notes are for South Australia ', 'role': 'user'}, {'content': 'SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "train_data = open_json(\"train_data.jsonl\")\n",
    "val_data = open_json(\"val_data.jsonl\")\n",
    "test_data = open_json(\"test_data.jsonl\")\n",
    "print(json.loads(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, BitsAndBytesConfig, get_scheduler, DataCollatorWithPadding, AutoModelForSeq2SeqLM\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import torch\n",
    "from accelerate import Accelerator, init_empty_weights, infer_auto_device_map\n",
    "\n",
    "# enable distributed training\n",
    "accelerate = Accelerator()\n",
    "\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# OOV token\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "# padding side\n",
    "tokenizer.pad_padding_side = \"right\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "device_map = {\"\":0}\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=bnb_config, device_map={\"\":0})\n",
    "# device_map = infer_auto_device_map(model, max_memory={\"cuda\": \"2GiB\", \"cpu\": \"16GiB\"}) \n",
    "# model = model.to(device_map)\n",
    "model = get_peft_model(model, bnb_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lora config\n",
    "lora_config = LoraConfig(\n",
    "    r = 256,\n",
    "    lora_alpha=128,\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = model.state_dict().keys()\n",
    "for name in layers:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"messages\"][\"content\"], return_tensors=\"pt\")\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "val_data = val_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 50\n",
    "num_train_steps = len(train_data) * num_epochs\n",
    "\n",
    "train_dataloader, val_dataloader, model, optimizer = accelerate.prepare(train_dataloader, val_dataloader, model, optimizer)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps,\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_train_steps))\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device_map) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "# save model\n",
    "model.save_pretrained(\"natural_language_to_sql\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "model_predict = \"natural_langauge_to_sql\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_predict, torch_dtype=torch.float16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
